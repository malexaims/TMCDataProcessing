{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa0cc77",
   "metadata": {},
   "source": [
    "\n",
    "# Turning Movement Counts — Intersection Analytics\n",
    "\n",
    "This notebook ingests **turning movement count (TMC)** data, parses it into a typed data model, and computes:\n",
    "- Seasonal adjustments\n",
    "- Growth factors (annual growth to a target year)\n",
    "- Peak-hour windows for **AM**, **Midday (MD)**, and **PM**\n",
    "- **Heavy vehicle (HV)** percentages\n",
    "- **Peak Hour Factors (PHF)** for each movement, each approach, and the full intersection, for arbitrary time periods\n",
    "\n",
    "> **Your file**: `SR 544 at Charlotte Rd (EXCEL EXPORT).xls` (already uploaded alongside this notebook). If you're running locally, put your file path in the `SOURCE_FILE` variable below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521871e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If your environment needs these, uncomment:\n",
    "# !pip install pandas numpy xlrd --quiet\n",
    "\n",
    "import math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Tuple, Optional, Literal\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "pd.options.display.float_format = \"{:,.3f}\".format\n",
    "\n",
    "SOURCE_FILE = \"SR 544 at Charlotte Rd (EXCEL EXPORT).xls\"  # Change if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad8a106",
   "metadata": {},
   "source": [
    "\n",
    "## Data model\n",
    "\n",
    "We represent an intersection with approaches (N, S, E, W) and movements (Left, Thru, Right). The **minimum required** columns are the movement volumes in 15‑minute bins; HV columns are optional but encouraged.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0a114",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Approach = Literal[\"NB\",\"SB\",\"EB\",\"WB\"]\n",
    "Movement = Literal[\"L\",\"T\",\"R\"]  # Left, Thru, Right\n",
    "\n",
    "def phf_from_series(q15: pd.Series) -> Optional[float]:\n",
    "    \"\"\"Compute PHF for a given series of 15-minute volumes that represents exactly one hour (4 bins).\n",
    "    PHF = Hourly volume / (4 * max 15-min volume). Returns None if not computable.\"\"\"\n",
    "    if q15 is None or q15.empty:\n",
    "        return None\n",
    "    if len(q15) != 4:\n",
    "        return None\n",
    "    hourly = float(q15.sum())\n",
    "    max_q = float(q15.max())\n",
    "    if max_q <= 0:\n",
    "        return None\n",
    "    return hourly / (4.0 * max_q)\n",
    "\n",
    "@dataclass\n",
    "class MovementCounts:\n",
    "    approach: Approach\n",
    "    movement: Movement  # L, T, or R\n",
    "    volumes_15min: pd.Series            # index: datetime-like, freq 15min, values: veh count\n",
    "    hv_15min: Optional[pd.Series]=None  # optional heavy vehicle 15-min counts\n",
    "\n",
    "    def total(self, start=None, end=None) -> float:\n",
    "        s = self.volumes_15min\n",
    "        if start or end:\n",
    "            s = s.loc[start:end]\n",
    "        return float(s.sum())\n",
    "\n",
    "    def hv_percent(self, start=None, end=None) -> Optional[float]:\n",
    "        if self.hv_15min is None:\n",
    "            return None\n",
    "        s = self.volumes_15min\n",
    "        h = self.hv_15min\n",
    "        if start or end:\n",
    "            s = s.loc[start:end]\n",
    "            h = h.loc[start:end]\n",
    "        tot = float(s.sum())\n",
    "        hv = float(h.sum())\n",
    "        if tot <= 0:\n",
    "            return None\n",
    "        return 100.0 * hv / tot\n",
    "\n",
    "@dataclass\n",
    "class ApproachCounts:\n",
    "    approach: Approach\n",
    "    movements: Dict[Movement, MovementCounts] = field(default_factory=dict)\n",
    "\n",
    "    def series(self, part: Literal[\"vol\",\"hv\"] = \"vol\") -> pd.Series:\n",
    "        frames = []\n",
    "        for m, mc in self.movements.items():\n",
    "            frames.append(mc.volumes_15min if part == \"vol\" else (mc.hv_15min or 0*mc.volumes_15min))\n",
    "        if not frames:\n",
    "            return pd.Series(dtype=float)\n",
    "        return sum(frames)\n",
    "\n",
    "    def total(self, start=None, end=None) -> float:\n",
    "        s = self.series(\"vol\")\n",
    "        if start or end:\n",
    "            s = s.loc[start:end]\n",
    "        return float(s.sum())\n",
    "\n",
    "    def hv_percent(self, start=None, end=None) -> Optional[float]:\n",
    "        s = self.series(\"vol\")\n",
    "        h = self.series(\"hv\")\n",
    "        if start or end:\n",
    "            s = s.loc[start:end]\n",
    "            h = h.loc[start:end]\n",
    "        tot = float(s.sum())\n",
    "        hv = float(h.sum())\n",
    "        if tot <= 0:\n",
    "            return None\n",
    "        return 100.0 * hv / tot\n",
    "\n",
    "@dataclass\n",
    "class Intersection:\n",
    "    name: str\n",
    "    interval_minutes: int\n",
    "    movements: Dict[Tuple[Approach, Movement], MovementCounts]  # key e.g., (\"NB\",\"L\")\n",
    "    meta: Dict = field(default_factory=dict)\n",
    "\n",
    "    @property\n",
    "    def index(self) -> pd.DatetimeIndex:\n",
    "        # Return the union index across movements (assumes aligned 15-min bins)\n",
    "        idx = None\n",
    "        for mc in self.movements.values():\n",
    "            idx = mc.volumes_15min.index if idx is None else idx.union(mc.volumes_15min.index)\n",
    "        return idx\n",
    "\n",
    "    def approach(self, approach: Approach) -> ApproachCounts:\n",
    "        mv = {m: mc for (a, m), mc in self.movements.items() if a == approach}\n",
    "        return ApproachCounts(approach=approach, movements=mv)\n",
    "\n",
    "    def as_dataframe(self) -> pd.DataFrame:\n",
    "        \"\"\"Return a wide DataFrame with columns like 'NB_L', 'NB_T', 'NB_R' and optional 'NB_L_HV', etc.\"\"\"\n",
    "        cols = {}\n",
    "        for (a, m), mc in self.movements.items():\n",
    "            cols[f\"{a}_{m}\"] = mc.volumes_15min\n",
    "            if mc.hv_15min is not None:\n",
    "                cols[f\"{a}_{m}_HV\"] = mc.hv_15min\n",
    "        df = pd.DataFrame(cols).sort_index()\n",
    "        return df\n",
    "\n",
    "    # --- Adjustments ---\n",
    "    def apply_seasonal(self, factors: Dict[str, float], period_map: Dict[pd.Timestamp, str]) -> \"Intersection\":\n",
    "        \"\"\"Return a new Intersection with volumes multiplied by seasonal factors keyed by period id (e.g., 'AM','MD','PM').\"\"\"\n",
    "        new = {}\n",
    "        for key, mc in self.movements.items():\n",
    "            vol = mc.volumes_15min.copy()\n",
    "            mult = vol.index.map(lambda ts: factors.get(period_map.get(ts, \"\"), 1.0))\n",
    "            vol = vol * pd.Series(mult, index=vol.index)\n",
    "            hv = None if mc.hv_15min is None else mc.hv_15min * pd.Series(mult, index=mc.hv_15min.index)\n",
    "            new[key] = MovementCounts(approach=mc.approach, movement=mc.movement, volumes_15min=vol, hv_15min=hv)\n",
    "        return Intersection(name=self.name+\" (seasonal)\", interval_minutes=self.interval_minutes, movements=new, meta=self.meta|{\"seasonal\": factors})\n",
    "\n",
    "    def apply_growth(self, annual_rate: float, years: float) -> \"Intersection\":\n",
    "        \"\"\"Compound growth: multiplier = (1 + r)^years.\"\"\"\n",
    "        g = (1.0 + annual_rate) ** years\n",
    "        new = {}\n",
    "        for key, mc in self.movements.items():\n",
    "            vol = mc.volumes_15min * g\n",
    "            hv  = None if mc.hv_15min is None else mc.hv_15min * g\n",
    "            new[key] = MovementCounts(approach=mc.approach, movement=mc.movement, volumes_15min=vol, hv_15min=hv)\n",
    "        return Intersection(name=self.name+f\" (grown {annual_rate:.3%} for {years:.2f}y)\", interval_minutes=self.interval_minutes, movements=new, meta=self.meta|{\"growth\": {\"annual_rate\": annual_rate, \"years\": years, \"multiplier\": g}})\n",
    "\n",
    "    # --- Peak windows & PHF ---\n",
    "    def find_peak_window(self, start: str, end: str) -> Tuple[pd.Timestamp, float]:\n",
    "        \"\"\"Within [start, end], find the 60-minute rolling window with the maximum *intersection* total volume (all movements).\n",
    "        Returns (window_start, total_volume).\"\"\"\n",
    "        df = self.as_dataframe().loc[start:end]\n",
    "        if df.empty:\n",
    "            return (pd.NaT, float(\"nan\"))\n",
    "        totals = df[[c for c in df.columns if not c.endswith(\"_HV\")]].sum(axis=1)\n",
    "        # sum across 4 consecutive 15-min bins\n",
    "        win = totals.rolling(window=4, min_periods=4).sum()\n",
    "        idx = win.idxmax()\n",
    "        return idx - pd.Timedelta(minutes=45), float(win.max())  # idx refers to end of window\n",
    "\n",
    "    def phf_by(self, level: Literal[\"movement\",\"approach\",\"intersection\"], window_start: pd.Timestamp) -> Dict[str, float]:\n",
    "        \"\"\"Compute PHF over the 60-min window starting at `window_start`.\n",
    "        Returns a dict mapping keys to PHF values.\"\"\"\n",
    "        t0 = pd.Timestamp(window_start)\n",
    "        t1 = t0 + pd.Timedelta(hours=1) - pd.Timedelta(minutes=self.interval_minutes)\n",
    "        phfs = {}\n",
    "        if level == \"movement\":\n",
    "            for (a,m), mc in self.movements.items():\n",
    "                q = mc.volumes_15min.loc[t0:t1]\n",
    "                phfs[f\"{a}_{m}\"] = phf_from_series(q)\n",
    "        elif level == \"approach\":\n",
    "            for a in [\"NB\",\"SB\",\"EB\",\"WB\"]:\n",
    "                s = self.approach(a).series(\"vol\").loc[t0:t1]\n",
    "                phfs[a] = phf_from_series(s)\n",
    "        elif level == \"intersection\":\n",
    "            s = self.as_dataframe().filter(regex=r\"^(?!.*_HV$).*$\").sum(axis=1).loc[t0:t1]\n",
    "            phfs[\"intersection\"] = phf_from_series(s)\n",
    "        return phfs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ecbd53",
   "metadata": {},
   "source": [
    "\n",
    "## Parsing your Excel\n",
    "\n",
    "FDOT/typical TMC exports vary. The parser below is **adapter-driven**: supply a **column map** from source columns to canonical keys like `NB_L`, `NB_T`, `NB_R`, etc., and (optionally) `NB_L_HV` for heavy vehicles.\n",
    "\n",
    "1. Set `SHEET` and `HEADER_ROW` to the sheet name and the zero-based header row of the time-series table.\n",
    "2. Provide `TIME_COLUMN` containing timestamps (or time-of-day).\n",
    "3. Fill `COLUMN_MAP` with your file's movement column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d968fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Configure these for your file ----\n",
    "SHEET = None   # e.g., 'Sheet1'; set to None to use the first sheet\n",
    "HEADER_ROW = 0 # 0-based row index where the time-series headers begin\n",
    "TIME_COLUMN = \"Time\"  # Name of the time column in the sheet\n",
    "\n",
    "# Example map. Replace values on the right with your exact column names from the sheet.\n",
    "COLUMN_MAP = {\n",
    "    # Movement volumes\n",
    "    \"NB_L\": \"NB Left\",\n",
    "    \"NB_T\": \"NB Thru\",\n",
    "    \"NB_R\": \"NB Right\",\n",
    "    \"SB_L\": \"SB Left\",\n",
    "    \"SB_T\": \"SB Thru\",\n",
    "    \"SB_R\": \"SB Right\",\n",
    "    \"EB_L\": \"EB Left\",\n",
    "    \"EB_T\": \"EB Thru\",\n",
    "    \"EB_R\": \"EB Right\",\n",
    "    \"WB_L\": \"WB Left\",\n",
    "    \"WB_T\": \"WB Thru\",\n",
    "    \"WB_R\": \"WB Right\",\n",
    "    # Optional HV columns (comment missing ones)\n",
    "    \"NB_L_HV\": \"NB Left HV\",\n",
    "    \"NB_T_HV\": \"NB Thru HV\",\n",
    "    \"NB_R_HV\": \"NB Right HV\",\n",
    "    \"SB_L_HV\": \"SB Left HV\",\n",
    "    \"SB_T_HV\": \"SB Thru HV\",\n",
    "    \"SB_R_HV\": \"SB Right HV\",\n",
    "    \"EB_L_HV\": \"EB Left HV\",\n",
    "    \"EB_T_HV\": \"EB Thru HV\",\n",
    "    \"EB_R_HV\": \"EB Right HV\",\n",
    "    \"WB_L_HV\": \"WB Left HV\",\n",
    "    \"WB_T_HV\": \"WB Thru HV\",\n",
    "    \"WB_R_HV\": \"WB Right HV\",\n",
    "}\n",
    "\n",
    "# Optional: if the sheet stores date and separate time columns, you can merge them here.\n",
    "DATE_FOR_COUNTS = None  # e.g., '2025-03-18' to anchor time-of-day into full timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ded047",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_tmc_dataframe(source_file: str = SOURCE_FILE,\n",
    "                       sheet: Optional[str] = SHEET,\n",
    "                       header_row: int = HEADER_ROW,\n",
    "                       time_column: str = TIME_COLUMN,\n",
    "                       column_map: Dict[str,str] = COLUMN_MAP) -> pd.DataFrame:\n",
    "    \"\"\"Read the Excel and return a canonical wide DataFrame indexed by timestamps at 15-min.\n",
    "    Columns will include keys from COLUMN_MAP where present.\"\"\"\n",
    "    df = pd.read_excel(source_file, sheet_name=sheet, header=header_row)\n",
    "    # Normalize time\n",
    "    if DATE_FOR_COUNTS is not None:\n",
    "        # time-of-day to full timestamps\n",
    "        ts = pd.to_datetime(DATE_FOR_COUNTS + \" \" + df[time_column].astype(str))\n",
    "    else:\n",
    "        ts = pd.to_datetime(df[time_column])\n",
    "    df.index = pd.DatetimeIndex(ts)\n",
    "    df = df.drop(columns=[time_column])\n",
    "    # Build canonical\n",
    "    out = pd.DataFrame(index=df.index)\n",
    "    for canon, src in column_map.items():\n",
    "        if src in df.columns:\n",
    "            out[canon] = pd.to_numeric(df[src], errors=\"coerce\").fillna(0).astype(float)\n",
    "    out = out.sort_index()\n",
    "    return out\n",
    "\n",
    "def build_intersection_from_df(name: str, wide_df: pd.DataFrame) -> Intersection:\n",
    "    # infer interval (minutes) from index\n",
    "    if len(wide_df.index) >= 2:\n",
    "        dt = (wide_df.index[1] - wide_df.index[0]).seconds // 60\n",
    "    else:\n",
    "        dt = 15\n",
    "    movements = {}\n",
    "    for approach in [\"NB\",\"SB\",\"EB\",\"WB\"]:\n",
    "        for mv in [\"L\",\"T\",\"R\"]:\n",
    "            key = f\"{approach}_{mv}\"\n",
    "            if key in wide_df.columns:\n",
    "                vol = wide_df[key].copy()\n",
    "                hv = wide_df.get(f\"{key}_HV\", None)\n",
    "                movements[(approach, mv)] = MovementCounts(approach=approach, movement=mv, volumes_15min=vol, hv_15min=hv)\n",
    "    return Intersection(name=name, interval_minutes=int(dt), movements=movements, meta={})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0c2b3",
   "metadata": {},
   "source": [
    "\n",
    "## Period definitions (AM / MD / PM) and seasonal factors\n",
    "\n",
    "Edit the period clock-times and factors as needed. The `period_map` assigns each timestamp to a period name (e.g., `\"AM\"`), then `apply_seasonal` uses `factors[period]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49b54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define daily periods — tweak as needed\n",
    "AM = (\"06:00\", \"10:00\")\n",
    "MD = (\"10:00\", \"15:00\")\n",
    "PM = (\"15:00\", \"19:00\")\n",
    "PERIODS = {\"AM\": AM, \"MD\": MD, \"PM\": PM}\n",
    "\n",
    "def build_period_map(index: pd.DatetimeIndex, periods: Dict[str, Tuple[str,str]] = PERIODS) -> Dict[pd.Timestamp, str]:\n",
    "    def in_range(t, start, end):\n",
    "        return (t >= pd.to_datetime(start).time()) and (t < pd.to_datetime(end).time())\n",
    "    m = {}\n",
    "    for ts in index:\n",
    "        label = \"\"\n",
    "        for name, (s,e) in periods.items():\n",
    "            if in_range(ts.time(), s, e):\n",
    "                label = name\n",
    "                break\n",
    "        m[ts] = label\n",
    "    return m\n",
    "\n",
    "# Example seasonal multipliers by period (1.00 = no change)\n",
    "SEASONAL_FACTORS = {\n",
    "    \"AM\": 1.05,\n",
    "    \"MD\": 0.98,\n",
    "    \"PM\": 1.03,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b4f583",
   "metadata": {},
   "source": [
    "\n",
    "## Load, build, and example analytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcd28ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Read the source; if your environment lacks 'xlrd' for .xls, install it above or save as .xlsx/.csv and change SOURCE_FILE.\n",
    "try:\n",
    "    wide = read_tmc_dataframe()\n",
    "except Exception as e:\n",
    "    print(\"Reading failed:\", e)\n",
    "    print(\"Tip: install xlrd for .xls:  !pip install xlrd  OR save to .xlsx/.csv and update SOURCE_FILE.\")\n",
    "    wide = pd.DataFrame()\n",
    "\n",
    "# 2) Build the Intersection model\n",
    "ix = build_intersection_from_df(\"SR 544 at Charlotte Rd\", wide) if not wide.empty else None\n",
    "\n",
    "# 3) Seasonal + growth examples\n",
    "if ix is not None:\n",
    "    pmap = build_period_map(ix.index)\n",
    "    ix_seasonal = ix.apply_seasonal(SEASONAL_FACTORS, pmap)\n",
    "    # Example: 2% annual growth for 5 years\n",
    "    ix_future = ix_seasonal.apply_growth(annual_rate=0.02, years=5.0)\n",
    "\n",
    "    display(ix.as_dataframe().head(8))\n",
    "else:\n",
    "    print(\"No intersection was built yet.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66fef19",
   "metadata": {},
   "source": [
    "\n",
    "## Peak windows and PHFs\n",
    "\n",
    "The `find_peak_window()` method scans a time range and returns the **start** of the highest 60‑minute intersection total. Then use `phf_by()` to compute PHFs for movements, approaches, or the whole intersection within that window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672e55f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def report_peak_and_phf(ix: Intersection, label: str, start_clock: str, end_clock: str):\n",
    "    if ix is None:\n",
    "        print(f\"[{label}] No data\")\n",
    "        return\n",
    "    day = ix.index[0].normalize()\n",
    "    start = pd.to_datetime(str(day.date()) + \" \" + start_clock)\n",
    "    end   = pd.to_datetime(str(day.date()) + \" \" + end_clock)\n",
    "    w0, vol = ix.find_peak_window(start=str(start), end=str(end))\n",
    "    if pd.isna(w0):\n",
    "        print(f\"[{label}] No window found in {start_clock}-{end_clock}\")\n",
    "        return\n",
    "    print(f\"[{label}] Peak window start: {w0}, total = {vol:,.0f}\")\n",
    "    phf_mov = ix.phf_by(\"movement\", w0)\n",
    "    phf_app = ix.phf_by(\"approach\", w0)\n",
    "    phf_int = ix.phf_by(\"intersection\", w0)\n",
    "    # Tabular views\n",
    "    df_mov = pd.DataFrame.from_dict(phf_mov, orient=\"index\", columns=[\"PHF\"]).sort_index()\n",
    "    df_app = pd.DataFrame.from_dict(phf_app, orient=\"index\", columns=[\"PHF\"]).sort_index()\n",
    "    df_int = pd.DataFrame.from_dict(phf_int, orient=\"index\", columns=[\"PHF\"]).sort_index()\n",
    "    display(df_mov)\n",
    "    display(df_app)\n",
    "    display(df_int)\n",
    "\n",
    "# Run standard AM/MD/PM\n",
    "if ix is not None:\n",
    "    report_peak_and_phf(ix, \"AM\", *AM)\n",
    "    report_peak_and_phf(ix, \"MD\", *MD)\n",
    "    report_peak_and_phf(ix, \"PM\", *PM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b685b6",
   "metadata": {},
   "source": [
    "\n",
    "## Heavy vehicle (HV) percentages\n",
    "\n",
    "If HV columns are provided (e.g., `NB_L_HV`), the notebook reports HV% by movement and approach for any period.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hv_percentages(ix: Intersection, start_clock: str, end_clock: str) -> pd.DataFrame:\n",
    "    if ix is None:\n",
    "        return pd.DataFrame()\n",
    "    day = ix.index[0].normalize()\n",
    "    start = pd.to_datetime(str(day.date()) + \" \" + start_clock)\n",
    "    end   = pd.to_datetime(str(day.date()) + \" \" + end_clock)\n",
    "    rows = []\n",
    "    for (a,m), mc in ix.movements.items():\n",
    "        pct = mc.hv_percent(start, end)\n",
    "        rows.append({\"key\": f\"{a}_{m}\", \"level\": \"movement\", \"HV_%\": pct})\n",
    "    for a in [\"NB\",\"SB\",\"EB\",\"WB\"]:\n",
    "        pct = ix.approach(a).hv_percent(start, end)\n",
    "        rows.append({\"key\": a, \"level\": \"approach\", \"HV_%\": pct})\n",
    "    df = pd.DataFrame(rows).set_index(\"key\").sort_index()\n",
    "    return df\n",
    "\n",
    "if ix is not None:\n",
    "    print(\"HV% — AM window:\")\n",
    "    display(hv_percentages(ix, *AM))\n",
    "    print(\"HV% — MD window:\")\n",
    "    display(hv_percentages(ix, *MD))\n",
    "    print(\"HV% — PM window:\")\n",
    "    display(hv_percentages(ix, *PM))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d2b86b",
   "metadata": {},
   "source": [
    "\n",
    "## Export\n",
    "\n",
    "You can export adjusted volumes and summary stats to CSV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd9aeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def export_intersection(ix: Intersection, path: str) -> None:\n",
    "    df = ix.as_dataframe()\n",
    "    df.to_csv(path, index=True)\n",
    "    print(f\"Wrote {path}\")\n",
    "\n",
    "# Example export (commented)\n",
    "# if ix is not None:\n",
    "#     export_intersection(ix_future, \"intersection_adjusted.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
